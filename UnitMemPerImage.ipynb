{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59d3997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff5c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_var_activations(var_model, input_tokens, target_layers=None):\n",
    "    \"\"\"\n",
    "    Extract activations from VAR model layers during forward pass\n",
    "    \n",
    "    Args:\n",
    "        var_model: The loaded VAR model\n",
    "        input_tokens: Input tokens for the model\n",
    "        target_layers: List of layer names to extract (if None, extracts all FFN layers)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of layer_name -> activation tensor\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    if target_layers is None:\n",
    "        # Extract all FFN layers by default\n",
    "        target_layers = []\n",
    "        for name, module in var_model.named_modules():\n",
    "            if 'ffn.fc1' in name or 'ffn.fc2' in name or name == 'head':\n",
    "                target_layers.append(name)\n",
    "    \n",
    "    def make_hook(layer_name):\n",
    "        def hook_fn(module, input, output):\n",
    "            # Store activation, handling different output shapes\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                if len(output.shape) == 3:  # [batch, sequence, features]\n",
    "                    # Average across sequence dimension for transformers\n",
    "                    activations[layer_name] = output.mean(dim=1).detach()\n",
    "                else:\n",
    "                    activations[layer_name] = output.detach()\n",
    "            else:\n",
    "                # Handle tuple outputs (some layers return multiple values)\n",
    "                activations[layer_name] = output[0].detach()\n",
    "        return hook_fn\n",
    "    \n",
    "    # Register hooks\n",
    "    for layer_name in target_layers:\n",
    "        try:\n",
    "            layer = dict(var_model.named_modules())[layer_name]\n",
    "            hook = layer.register_forward_hook(make_hook(layer_name))\n",
    "            hooks.append(hook)\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Layer {layer_name} not found\")\n",
    "    \n",
    "    # Perform forward pass\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = var_model(torch.tensor([1,]), input_tokens)\n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    \n",
    "    return activations\n",
    "\n",
    "\n",
    "def extract_all_linear_activations(var_model, input_tokens):\n",
    "    \"\"\"\n",
    "    Extract activations from all Linear (fully connected) layers in the VAR model.\n",
    "\n",
    "    Args:\n",
    "        var_model: The loaded VAR model\n",
    "        input_tokens: Input tokens for the model\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of layer_name -> activation tensor\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "\n",
    "    # Find all Linear layers\n",
    "    for name, module in var_model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            def make_hook(layer_name):\n",
    "                def hook_fn(module, input, output):\n",
    "                    if isinstance(output, torch.Tensor):\n",
    "                        if len(output.shape) == 3:\n",
    "                            activations[layer_name] = output.mean(dim=1).detach()\n",
    "                        else:\n",
    "                            activations[layer_name] = output.detach()\n",
    "                    else:\n",
    "                        activations[layer_name] = output[0].detach()\n",
    "                return hook_fn\n",
    "            hook = module.register_forward_hook(make_hook(name))\n",
    "            hooks.append(hook)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = var_model(torch.tensor([1,]), input_tokens)\n",
    "    finally:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07778fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Create dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root='./data/imagenette2/',\n",
    "    transform=transform\n",
    ")\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a32c0",
   "metadata": {},
   "source": [
    "## Iterate over different images and collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb29ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Aug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m this_imgs_activations = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):  \u001b[38;5;66;03m# Iterate over different random augmentations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     image = \u001b[43mAug\u001b[49m(image)  \u001b[38;5;66;03m# Apply random augmentation\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     14\u001b[39m         \u001b[38;5;66;03m# This depends on your VAE implementation\u001b[39;00m\n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# You may need to adjust based on your specific VAE interface\u001b[39;00m\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m#encoded_tokens = vae.encode(image_tensor)  # Adjust this line\u001b[39;00m\n\u001b[32m     17\u001b[39m         vae_out = vae.img_to_idxBl(image)\n",
      "\u001b[31mNameError\u001b[39m: name 'Aug' is not defined"
     ]
    }
   ],
   "source": [
    "nsamples = 12\n",
    "all_means = []\n",
    "for i, image in enumerate(dataloader): \n",
    "    print(i)\n",
    "    if i >= nsamples:\n",
    "        break\n",
    "    image = image[0]\n",
    "    print(image.shape)\n",
    "    image = image.to(device)\n",
    "    this_imgs_activations = []\n",
    "    for _ in range(10):  # Iterate over different random augmentations\n",
    "        image = Aug(image)  # Apply random augmentation\n",
    "        with torch.no_grad():\n",
    "            # This depends on your VAE implementation\n",
    "            # You may need to adjust based on your specific VAE interface\n",
    "            #encoded_tokens = vae.encode(image_tensor)  # Adjust this line\n",
    "            vae_out = vae.img_to_idxBl(image)\n",
    "            var_in = var.vae_quant_proxy[0].idxBl_to_var_input(vae_out)    \n",
    "        activations = extract_all_linear_activations(var, var_in)\n",
    "        this_imgs_activations.append(activations)\n",
    "    # Compute the mean activation across all augmentations for each layer in all_activations\n",
    "    layer_means = {}\n",
    "    if this_imgs_activations:\n",
    "        # Get all layer names from the first activation dict\n",
    "        layer_names = this_imgs_activations[0].keys()\n",
    "        for lname in layer_names:\n",
    "            # Stack tensors for this layer across all augmentations\n",
    "            stacked = torch.stack([a[lname] for a in this_imgs_activations], dim=0)\n",
    "            # Compute mean across augmentations (dim=0)\n",
    "            layer_means[lname] = stacked.mean(dim=0)\n",
    "            #print(f\"Layer: {lname}, Mean activation shape: {layer_means[lname].shape}\")\n",
    "    all_means.append(layer_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc04bfe",
   "metadata": {},
   "source": [
    "## Data analysis part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ad3b7",
   "metadata": {},
   "source": [
    "#### Compute the max activation across all units for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aced293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each layer, stack all mean activations across images and compute the maximum per unit\n",
    "max_per_unit_all_layers = {}\n",
    "\n",
    "# Assume all_means is a list of dicts, each dict: layer_name -> mean activation tensor\n",
    "if all_means:\n",
    "    layer_names = all_means[0].keys()\n",
    "    for lname in layer_names:\n",
    "        # Stack all mean activations for this layer across images\n",
    "        stacked_means = torch.stack([mean[lname] for mean in all_means], dim=0)\n",
    "        # Compute the maximum for each unit across all images (dim=0)\n",
    "        max_per_unit_all_layers[lname] = stacked_means.max(dim=0).values\n",
    "        print(f\"Maximum per unit in layer {lname}:\")\n",
    "        print(max_per_unit_all_layers[lname])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc10531",
   "metadata": {},
   "source": [
    "#### compute max and argmax for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6299389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each layer, stack all mean activations across images and compute the maximum and argmax per unit\n",
    "max_per_unit_all_layers = {}\n",
    "argmax_per_unit_all_layers = {}\n",
    "\n",
    "if all_means:\n",
    "    layer_names = all_means[0].keys()\n",
    "    for lname in layer_names:\n",
    "        # Stack all mean activations for this layer across images\n",
    "        stacked_means = torch.stack([mean[lname] for mean in all_means], dim=0)\n",
    "        # Compute the maximum and argmax for each unit across all images (dim=0)\n",
    "        stacked_means = stacked_means.abs()\n",
    "        max_per_unit_all_layers[lname] = stacked_means.max(dim=0).values\n",
    "        argmax_per_unit_all_layers[lname] = stacked_means.argmax(dim=0)\n",
    "        print(f\"Layer: {lname}\")\n",
    "        print(f\"Maximum per unit: {max_per_unit_all_layers[lname]}\")\n",
    "        print(f\"Argmax per unit: {argmax_per_unit_all_layers[lname]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e195f",
   "metadata": {},
   "source": [
    "#### compute $\\mu_{-max}$, i.e. the nominator of the unit mem metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d830f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each layer, compute the mean activation across all images except the one at argmax for each unit\n",
    "mean_minus_max_per_unit_all_layers = {}\n",
    "\n",
    "if all_means:\n",
    "    layer_names = all_means[0].keys()\n",
    "    n_imgs = len(all_means)\n",
    "    for lname in layer_names:\n",
    "        # Stack all mean activations for this layer across images: shape [n_imgs, 1, n_units]\n",
    "        stacked_means = torch.stack([mean[lname] for mean in all_means], dim=0)\n",
    "        # Find argmax index for each unit\n",
    "        stacked_means = stacked_means.abs()\n",
    "        argmax_idx = stacked_means.argmax(dim=0)  # shape [1, n_units]\n",
    "        # For each unit, exclude the image at argmax and compute mean over the rest\n",
    "        means = []\n",
    "        for unit in range(stacked_means.shape[2]):\n",
    "            # Get indices of all images except the one at argmax for this unit\n",
    "            mask = torch.ones(n_imgs, dtype=torch.bool)\n",
    "            mask[argmax_idx[0, unit]] = False\n",
    "            # Compute mean over the remaining images for this unit\n",
    "            mean_val = stacked_means[mask, 0, unit].mean()\n",
    "            means.append(mean_val)\n",
    "        mean_minus_max_per_unit_all_layers[lname] = torch.stack(means)\n",
    "        print(f\"Layer: {lname}, Âµ-max per unit shape: {mean_minus_max_per_unit_all_layers[lname]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d66ba8",
   "metadata": {},
   "source": [
    "#### compute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff941a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference between max_per_unit_all_layers and mean_minus_max_per_unit_all_layers for each layer\n",
    "diff_max_minus_mean_all_layers = {}\n",
    "\n",
    "for lname in max_per_unit_all_layers.keys():\n",
    "    diff_max_minus_mean_all_layers[lname] = max_per_unit_all_layers[lname] - mean_minus_max_per_unit_all_layers[lname]\n",
    "    print(f\"Layer: {lname}, Difference shape: {diff_max_minus_mean_all_layers[lname].shape}\")\n",
    "    print(diff_max_minus_mean_all_layers[lname])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52ce78",
   "metadata": {},
   "source": [
    "#### compute unit mem metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83616ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized difference: (max - mean) / (max + mean) for each layer\n",
    "normalized_diff_all_layers = {}\n",
    "\n",
    "for lname in diff_max_minus_mean_all_layers.keys():\n",
    "    max_vals = max_per_unit_all_layers[lname]\n",
    "    mean_vals = mean_minus_max_per_unit_all_layers[lname]\n",
    "    denom = max_vals + mean_vals\n",
    "    # Avoid division by zero\n",
    "    normalized_diff = diff_max_minus_mean_all_layers[lname] / (denom + 1e-8)\n",
    "    normalized_diff_all_layers[lname] = normalized_diff\n",
    "    print(f\"Layer: {lname}, Normalized difference shape: {normalized_diff.shape}\")\n",
    "    print(normalized_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
