{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ðŸš€ For an interactive experience, head over to our [demo platform](https://var.vision/demo) and dive right in! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)\n",
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights into the VAR model.\n",
      "Model is ready for inference.\n"
     ]
    }
   ],
   "source": [
    "var_checkpoint_path = 'var_d16.pth'\n",
    "\n",
    "# Load the state dictionary directly, as identified previously\n",
    "state_dict = torch.load(var_checkpoint_path, map_location='cpu')\n",
    "var.load_state_dict(state_dict, strict=True)\n",
    "print(\"Successfully loaded weights into the VAR model.\")\n",
    "\n",
    "# --- 4. Prepare for Inference ---\n",
    "# Set the model to evaluation mode to disable dropout, etc.\n",
    "var.eval()\n",
    "\n",
    "# The model's parameters should not require gradients for inference\n",
    "for p in var.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"Model is ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAR(\n",
      "  drop_path_rate=0.0666667\n",
      "  (word_embed): Linear(in_features=32, out_features=1024, bias=True)\n",
      "  (class_emb): Embedding(1001, 1024)\n",
      "  (lvl_embed): Embedding(10, 1024)\n",
      "  (shared_ada_lin): Identity()\n",
      "  (blocks): ModuleList(\n",
      "    (0): AdaLNSelfAttn(\n",
      "      shared_aln=False\n",
      "      (drop_path): Identity()\n",
      "      (attn): SelfAttention(\n",
      "        using_flash=False, using_xform=False, attn_l2_norm=True\n",
      "        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Identity()\n",
      "      )\n",
      "      (ffn): FFN(\n",
      "        fused_mlp_func=False\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='tanh')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Identity()\n",
      "      )\n",
      "      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "      (ada_lin): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1-15): 15 x AdaLNSelfAttn(\n",
      "      shared_aln=False\n",
      "      (drop_path): DropPath((drop_prob=...))\n",
      "      (attn): SelfAttention(\n",
      "        using_flash=False, using_xform=False, attn_l2_norm=True\n",
      "        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Identity()\n",
      "      )\n",
      "      (ffn): FFN(\n",
      "        fused_mlp_func=False\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='tanh')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Identity()\n",
      "      )\n",
      "      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "      (ada_lin): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head_nm): AdaLNBeforeHead(\n",
      "    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
      "    (ada_lin): Sequential(\n",
      "      (0): SiLU()\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=1024, out_features=4096, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQVAE(\n",
      "  (encoder): Encoder(\n",
      "    (conv_in): Conv2d(3, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down): ModuleList(\n",
      "      (0-1): 2 x Module(\n",
      "        (block): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample2x(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (2): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample2x(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (3): Module(\n",
      "        (block): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (downsample): Downsample2x(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (4): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList(\n",
      "          (0-1): 2 x AttnBlock(\n",
      "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (qkv): Conv2d(640, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid): Module(\n",
      "      (block_1): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (dropout): Identity()\n",
      "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (attn_1): AttnBlock(\n",
      "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (qkv): Conv2d(640, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (block_2): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (dropout): Identity()\n",
      "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm_out): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "    (conv_out): Conv2d(640, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_in): Conv2d(32, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (mid): Module(\n",
      "      (block_1): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (dropout): Identity()\n",
      "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (attn_1): AttnBlock(\n",
      "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (qkv): Conv2d(640, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (block_2): ResnetBlock(\n",
      "        (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "        (dropout): Identity()\n",
      "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "    )\n",
      "    (up): ModuleList(\n",
      "      (0): Module(\n",
      "        (block): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "      )\n",
      "      (1): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample2x(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (2): Module(\n",
      "        (block): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample2x(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (3): Module(\n",
      "        (block): ModuleList(\n",
      "          (0): ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList()\n",
      "        (upsample): Upsample2x(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (4): Module(\n",
      "        (block): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Identity()\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nin_shortcut): Identity()\n",
      "          )\n",
      "        )\n",
      "        (attn): ModuleList(\n",
      "          (0-2): 3 x AttnBlock(\n",
      "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (qkv): Conv2d(640, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (upsample): Upsample2x(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): GroupNorm(32, 160, eps=1e-06, affine=True)\n",
      "    (conv_out): Conv2d(160, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (quantize): VectorQuantizer2(\n",
      "    (1, 2, 3, 4, 5, 6, 8, 10, 13, 16), znorm=False, beta=0.25  |  S=10, quant_resi=0.5\n",
      "    (quant_resi): PhiPartiallyShared(\n",
      "      ticks=[0.08333333 0.36111111 0.63888889 0.91666667]\n",
      "      (qresi_ls): ModuleList(\n",
      "        (0-3): 4 x Phi(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(4096, 32)\n",
      "  )\n",
      "  (quant_conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (post_quant_conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Type: <class 'models.var.VAR'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Layer Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'models.basic_var.AdaLNBeforeHead'>\n",
      "Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in var.named_modules():\n",
    "        print(f\"Layer Type: {type(module)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Name: pos_start, Parameter Shape: torch.Size([1, 1, 1024])\n",
      "Parameter Name: pos_1LC, Parameter Shape: torch.Size([1, 680, 1024])\n",
      "Parameter Name: word_embed.weight, Parameter Shape: torch.Size([1024, 32])\n",
      "Parameter Name: word_embed.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: class_emb.weight, Parameter Shape: torch.Size([1001, 1024])\n",
      "Parameter Name: lvl_embed.weight, Parameter Shape: torch.Size([10, 1024])\n",
      "Parameter Name: blocks.0.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.0.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.0.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.0.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.0.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.0.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.0.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.0.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.0.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.0.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.0.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.0.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.1.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.1.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.1.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.1.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.1.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.1.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.1.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.1.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.1.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.1.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.1.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.1.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.2.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.2.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.2.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.2.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.2.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.2.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.2.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.2.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.2.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.2.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.2.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.2.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.3.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.3.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.3.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.3.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.3.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.3.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.3.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.3.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.3.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.3.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.3.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.3.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.4.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.4.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.4.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.4.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.4.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.4.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.4.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.4.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.4.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.4.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.4.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.4.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.5.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.5.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.5.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.5.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.5.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.5.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.5.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.5.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.5.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.5.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.5.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.5.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.6.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.6.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.6.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.6.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.6.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.6.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.6.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.6.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.6.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.6.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.6.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.6.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.7.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.7.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.7.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.7.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.7.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.7.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.7.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.7.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.7.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.7.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.7.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.7.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.8.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.8.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.8.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.8.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.8.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.8.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.8.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.8.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.8.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.8.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.8.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.8.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.9.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.9.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.9.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.9.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.9.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.9.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.9.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.9.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.9.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.9.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.9.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.9.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.10.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.10.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.10.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.10.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.10.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.10.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.10.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.10.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.10.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.10.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.10.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.10.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.11.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.11.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.11.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.11.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.11.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.11.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.11.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.11.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.11.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.11.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.11.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.11.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.12.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.12.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.12.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.12.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.12.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.12.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.12.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.12.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.12.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.12.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.12.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.12.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.13.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.13.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.13.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.13.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.13.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.13.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.13.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.13.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.13.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.13.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.13.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.13.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.14.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.14.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.14.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.14.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.14.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.14.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.14.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.14.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.14.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.14.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.14.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.14.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: blocks.15.attn.scale_mul_1H11, Parameter Shape: torch.Size([1, 16, 1, 1])\n",
      "Parameter Name: blocks.15.attn.q_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.15.attn.v_bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.15.attn.mat_qkv.weight, Parameter Shape: torch.Size([3072, 1024])\n",
      "Parameter Name: blocks.15.attn.proj.weight, Parameter Shape: torch.Size([1024, 1024])\n",
      "Parameter Name: blocks.15.attn.proj.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.15.ffn.fc1.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: blocks.15.ffn.fc1.bias, Parameter Shape: torch.Size([4096])\n",
      "Parameter Name: blocks.15.ffn.fc2.weight, Parameter Shape: torch.Size([1024, 4096])\n",
      "Parameter Name: blocks.15.ffn.fc2.bias, Parameter Shape: torch.Size([1024])\n",
      "Parameter Name: blocks.15.ada_lin.1.weight, Parameter Shape: torch.Size([6144, 1024])\n",
      "Parameter Name: blocks.15.ada_lin.1.bias, Parameter Shape: torch.Size([6144])\n",
      "Parameter Name: head_nm.ada_lin.1.weight, Parameter Shape: torch.Size([2048, 1024])\n",
      "Parameter Name: head_nm.ada_lin.1.bias, Parameter Shape: torch.Size([2048])\n",
      "Parameter Name: head.weight, Parameter Shape: torch.Size([4096, 1024])\n",
      "Parameter Name: head.bias, Parameter Shape: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for name, param in var.named_parameters():\n",
    "    print(\"Parameter Name: \" + name + \", Parameter Shape: \" + str(param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: word_embed, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: class_emb, Layer Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Layer Name: lvl_embed, Layer Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Layer Name: shared_ada_lin, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: blocks, Layer Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "Layer Name: 0, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 1, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 2, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 3, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 4, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 5, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 6, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 7, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 8, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 9, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 10, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 11, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 12, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 13, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 14, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: 15, Layer Type: <class 'models.basic_var.AdaLNSelfAttn'>\n",
      "Layer Name: drop_path, Layer Type: <class 'models.helpers.DropPath'>\n",
      "Layer Name: attn, Layer Type: <class 'models.basic_var.SelfAttention'>\n",
      "Layer Name: mat_qkv, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: proj_drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ffn, Layer Type: <class 'models.basic_var.FFN'>\n",
      "Layer Name: fc1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: act, Layer Type: <class 'torch.nn.modules.activation.GELU'>\n",
      "Layer Name: fc2, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: drop, Layer Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: head_nm, Layer Type: <class 'models.basic_var.AdaLNBeforeHead'>\n",
      "Layer Name: ln_wo_grad, Layer Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Layer Name: ada_lin, Layer Type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer Name: 0, Layer Type: <class 'torch.nn.modules.activation.SiLU'>\n",
      "Layer Name: 1, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer Name: head, Layer Type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom initialization function\n",
    "def custom_init(layer):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Example neural network model with nested structures\n",
    "class NestedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NestedCNN, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "model = var\n",
    "model.apply(custom_init)\n",
    "\n",
    "# Function to recursively iterate over all layers\n",
    "def iterate_layers(module):\n",
    "    for name, layer in module.named_children():\n",
    "        print(f\"Layer Name: {name}, Layer Type: {type(layer)}\")\n",
    "        iterate_layers(layer)  # Recursively iterate over nested layers\n",
    "iterate_layers(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m final = []\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     out = \u001b[43mnew_m\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out.items():\n\u001b[32m     42\u001b[39m         my = np.mean(v.reshape(\u001b[32m256\u001b[39m, \u001b[32m4\u001b[39m).cpu().detach().numpy(), axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:69\u001b[39m, in \u001b[36mIntermediateLayerGetter.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     67\u001b[39m out = OrderedDict()\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items():\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     x = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_layers:\n\u001b[32m     71\u001b[39m         out_name = \u001b[38;5;28mself\u001b[39m.return_layers[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "### UNIT MEM MEASUREMENT\n",
    "datapath = './data'\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "s = 1\n",
    "color_jitter = transforms.ColorJitter(\n",
    "        0.9 * s, 0.9 * s, 0.9 * s, 0.1 * s)\n",
    "flip = transforms.RandomHorizontalFlip()\n",
    "Aug = transforms.Compose(\n",
    "    [\n",
    "    transforms.RandomResizedCrop(size=32),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomApply([color_jitter], p=0.9),\n",
    "    transforms.RandomGrayscale(p=0.1)\n",
    "    ])\n",
    "data_transforms = transforms.Compose(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                Normalize(0.5, 0.5)\n",
    "            ])\n",
    "CIFAR_10_Dataset = torchvision.datasets.CIFAR10(datapath, train=True, download=True,\n",
    "                                                 transform=data_transforms)\n",
    "sublist = list(range(0, 2, 1))\n",
    "subset = torch.utils.data.Subset(CIFAR_10_Dataset, sublist)\n",
    "dataloader = torch.utils.data.DataLoader(subset, 1, shuffle=False, num_workers=2)\n",
    "\n",
    "#model.load_state_dict(torch.load('./var_d16.pth'))\n",
    "new_m = torchvision.models._utils.IntermediateLayerGetter(var, {'head': 'feat1'})\n",
    "\n",
    "final1 = []\n",
    "if __name__ == '__main__':\n",
    "    for img, label in tqdm(iter(dataloader)):\n",
    "        final = []\n",
    "        for j in range(10):\n",
    "            out = new_m(Aug(img))\n",
    "            for k, v in out.items():\n",
    "                my = np.mean(v.reshape(256, 4).cpu().detach().numpy(), axis=1)\n",
    "                final.append(my)\n",
    "        out1 = np.mean(np.array(final), axis=0)\n",
    "        final1.append(out1)\n",
    "\n",
    "    finalout = np.array(final1)\n",
    "    maxout = np.max(finalout, axis=0)\n",
    "    medianout = np.median(np.sort(finalout, axis=0)[0:-1], axis=0)\n",
    "    selectivity = (maxout - medianout)/(maxout + medianout)\n",
    "    scipy.io.savemat('./data/selectivity_unit.mat', {'selectivity': selectivity})\n",
    "\n",
    "    # Top 10% der Neuronen mit hÃ¶chster Selectivity finden\n",
    "    num_neurons = selectivity.shape[0]\n",
    "    top_k = int(np.ceil(num_neurons * 0.1))\n",
    "    top_indices = np.argsort(selectivity)[-top_k:][::-1]  # absteigend sortiert\n",
    "    print(f\"Top 10% Neuronen-Indizes: {top_indices}\")\n",
    "    # Optional: als .mat speichern\n",
    "    scipy.io.savemat('./data/top10percent_unit_indices.mat', {'top10percent_indices': top_indices})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAR(\n",
       "  drop_path_rate=0.0666667\n",
       "  (word_embed): Linear(in_features=32, out_features=1024, bias=True)\n",
       "  (class_emb): Embedding(1001, 1024)\n",
       "  (lvl_embed): Embedding(10, 1024)\n",
       "  (shared_ada_lin): Identity()\n",
       "  (blocks): ModuleList(\n",
       "    (0): AdaLNSelfAttn(\n",
       "      shared_aln=False\n",
       "      (drop_path): Identity()\n",
       "      (attn): SelfAttention(\n",
       "        using_flash=False, using_xform=False, attn_l2_norm=True\n",
       "        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Identity()\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        fused_mlp_func=False\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Identity()\n",
       "      )\n",
       "      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (ada_lin): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1-15): 15 x AdaLNSelfAttn(\n",
       "      shared_aln=False\n",
       "      (drop_path): DropPath((drop_prob=...))\n",
       "      (attn): SelfAttention(\n",
       "        using_flash=False, using_xform=False, attn_l2_norm=True\n",
       "        (mat_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Identity()\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        fused_mlp_func=False\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Identity()\n",
       "      )\n",
       "      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (ada_lin): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head_nm): AdaLNBeforeHead(\n",
       "    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "    (ada_lin): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=1024, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = var\n",
    "model.apply(custom_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_embed\n",
      "class_emb\n",
      "lvl_embed\n",
      "shared_ada_lin\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.drop_path\n",
      "blocks.0.attn\n",
      "blocks.0.attn.mat_qkv\n",
      "blocks.0.attn.proj\n",
      "blocks.0.attn.proj_drop\n",
      "blocks.0.ffn\n",
      "blocks.0.ffn.fc1\n",
      "blocks.0.ffn.act\n",
      "blocks.0.ffn.fc2\n",
      "blocks.0.ffn.drop\n",
      "blocks.0.ln_wo_grad\n",
      "blocks.0.ada_lin\n",
      "blocks.0.ada_lin.0\n",
      "blocks.0.ada_lin.1\n",
      "blocks.1\n",
      "blocks.1.drop_path\n",
      "blocks.1.attn\n",
      "blocks.1.attn.mat_qkv\n",
      "blocks.1.attn.proj\n",
      "blocks.1.attn.proj_drop\n",
      "blocks.1.ffn\n",
      "blocks.1.ffn.fc1\n",
      "blocks.1.ffn.act\n",
      "blocks.1.ffn.fc2\n",
      "blocks.1.ffn.drop\n",
      "blocks.1.ln_wo_grad\n",
      "blocks.1.ada_lin\n",
      "blocks.1.ada_lin.0\n",
      "blocks.1.ada_lin.1\n",
      "blocks.2\n",
      "blocks.2.drop_path\n",
      "blocks.2.attn\n",
      "blocks.2.attn.mat_qkv\n",
      "blocks.2.attn.proj\n",
      "blocks.2.attn.proj_drop\n",
      "blocks.2.ffn\n",
      "blocks.2.ffn.fc1\n",
      "blocks.2.ffn.act\n",
      "blocks.2.ffn.fc2\n",
      "blocks.2.ffn.drop\n",
      "blocks.2.ln_wo_grad\n",
      "blocks.2.ada_lin\n",
      "blocks.2.ada_lin.0\n",
      "blocks.2.ada_lin.1\n",
      "blocks.3\n",
      "blocks.3.drop_path\n",
      "blocks.3.attn\n",
      "blocks.3.attn.mat_qkv\n",
      "blocks.3.attn.proj\n",
      "blocks.3.attn.proj_drop\n",
      "blocks.3.ffn\n",
      "blocks.3.ffn.fc1\n",
      "blocks.3.ffn.act\n",
      "blocks.3.ffn.fc2\n",
      "blocks.3.ffn.drop\n",
      "blocks.3.ln_wo_grad\n",
      "blocks.3.ada_lin\n",
      "blocks.3.ada_lin.0\n",
      "blocks.3.ada_lin.1\n",
      "blocks.4\n",
      "blocks.4.drop_path\n",
      "blocks.4.attn\n",
      "blocks.4.attn.mat_qkv\n",
      "blocks.4.attn.proj\n",
      "blocks.4.attn.proj_drop\n",
      "blocks.4.ffn\n",
      "blocks.4.ffn.fc1\n",
      "blocks.4.ffn.act\n",
      "blocks.4.ffn.fc2\n",
      "blocks.4.ffn.drop\n",
      "blocks.4.ln_wo_grad\n",
      "blocks.4.ada_lin\n",
      "blocks.4.ada_lin.0\n",
      "blocks.4.ada_lin.1\n",
      "blocks.5\n",
      "blocks.5.drop_path\n",
      "blocks.5.attn\n",
      "blocks.5.attn.mat_qkv\n",
      "blocks.5.attn.proj\n",
      "blocks.5.attn.proj_drop\n",
      "blocks.5.ffn\n",
      "blocks.5.ffn.fc1\n",
      "blocks.5.ffn.act\n",
      "blocks.5.ffn.fc2\n",
      "blocks.5.ffn.drop\n",
      "blocks.5.ln_wo_grad\n",
      "blocks.5.ada_lin\n",
      "blocks.5.ada_lin.0\n",
      "blocks.5.ada_lin.1\n",
      "blocks.6\n",
      "blocks.6.drop_path\n",
      "blocks.6.attn\n",
      "blocks.6.attn.mat_qkv\n",
      "blocks.6.attn.proj\n",
      "blocks.6.attn.proj_drop\n",
      "blocks.6.ffn\n",
      "blocks.6.ffn.fc1\n",
      "blocks.6.ffn.act\n",
      "blocks.6.ffn.fc2\n",
      "blocks.6.ffn.drop\n",
      "blocks.6.ln_wo_grad\n",
      "blocks.6.ada_lin\n",
      "blocks.6.ada_lin.0\n",
      "blocks.6.ada_lin.1\n",
      "blocks.7\n",
      "blocks.7.drop_path\n",
      "blocks.7.attn\n",
      "blocks.7.attn.mat_qkv\n",
      "blocks.7.attn.proj\n",
      "blocks.7.attn.proj_drop\n",
      "blocks.7.ffn\n",
      "blocks.7.ffn.fc1\n",
      "blocks.7.ffn.act\n",
      "blocks.7.ffn.fc2\n",
      "blocks.7.ffn.drop\n",
      "blocks.7.ln_wo_grad\n",
      "blocks.7.ada_lin\n",
      "blocks.7.ada_lin.0\n",
      "blocks.7.ada_lin.1\n",
      "blocks.8\n",
      "blocks.8.drop_path\n",
      "blocks.8.attn\n",
      "blocks.8.attn.mat_qkv\n",
      "blocks.8.attn.proj\n",
      "blocks.8.attn.proj_drop\n",
      "blocks.8.ffn\n",
      "blocks.8.ffn.fc1\n",
      "blocks.8.ffn.act\n",
      "blocks.8.ffn.fc2\n",
      "blocks.8.ffn.drop\n",
      "blocks.8.ln_wo_grad\n",
      "blocks.8.ada_lin\n",
      "blocks.8.ada_lin.0\n",
      "blocks.8.ada_lin.1\n",
      "blocks.9\n",
      "blocks.9.drop_path\n",
      "blocks.9.attn\n",
      "blocks.9.attn.mat_qkv\n",
      "blocks.9.attn.proj\n",
      "blocks.9.attn.proj_drop\n",
      "blocks.9.ffn\n",
      "blocks.9.ffn.fc1\n",
      "blocks.9.ffn.act\n",
      "blocks.9.ffn.fc2\n",
      "blocks.9.ffn.drop\n",
      "blocks.9.ln_wo_grad\n",
      "blocks.9.ada_lin\n",
      "blocks.9.ada_lin.0\n",
      "blocks.9.ada_lin.1\n",
      "blocks.10\n",
      "blocks.10.drop_path\n",
      "blocks.10.attn\n",
      "blocks.10.attn.mat_qkv\n",
      "blocks.10.attn.proj\n",
      "blocks.10.attn.proj_drop\n",
      "blocks.10.ffn\n",
      "blocks.10.ffn.fc1\n",
      "blocks.10.ffn.act\n",
      "blocks.10.ffn.fc2\n",
      "blocks.10.ffn.drop\n",
      "blocks.10.ln_wo_grad\n",
      "blocks.10.ada_lin\n",
      "blocks.10.ada_lin.0\n",
      "blocks.10.ada_lin.1\n",
      "blocks.11\n",
      "blocks.11.drop_path\n",
      "blocks.11.attn\n",
      "blocks.11.attn.mat_qkv\n",
      "blocks.11.attn.proj\n",
      "blocks.11.attn.proj_drop\n",
      "blocks.11.ffn\n",
      "blocks.11.ffn.fc1\n",
      "blocks.11.ffn.act\n",
      "blocks.11.ffn.fc2\n",
      "blocks.11.ffn.drop\n",
      "blocks.11.ln_wo_grad\n",
      "blocks.11.ada_lin\n",
      "blocks.11.ada_lin.0\n",
      "blocks.11.ada_lin.1\n",
      "blocks.12\n",
      "blocks.12.drop_path\n",
      "blocks.12.attn\n",
      "blocks.12.attn.mat_qkv\n",
      "blocks.12.attn.proj\n",
      "blocks.12.attn.proj_drop\n",
      "blocks.12.ffn\n",
      "blocks.12.ffn.fc1\n",
      "blocks.12.ffn.act\n",
      "blocks.12.ffn.fc2\n",
      "blocks.12.ffn.drop\n",
      "blocks.12.ln_wo_grad\n",
      "blocks.12.ada_lin\n",
      "blocks.12.ada_lin.0\n",
      "blocks.12.ada_lin.1\n",
      "blocks.13\n",
      "blocks.13.drop_path\n",
      "blocks.13.attn\n",
      "blocks.13.attn.mat_qkv\n",
      "blocks.13.attn.proj\n",
      "blocks.13.attn.proj_drop\n",
      "blocks.13.ffn\n",
      "blocks.13.ffn.fc1\n",
      "blocks.13.ffn.act\n",
      "blocks.13.ffn.fc2\n",
      "blocks.13.ffn.drop\n",
      "blocks.13.ln_wo_grad\n",
      "blocks.13.ada_lin\n",
      "blocks.13.ada_lin.0\n",
      "blocks.13.ada_lin.1\n",
      "blocks.14\n",
      "blocks.14.drop_path\n",
      "blocks.14.attn\n",
      "blocks.14.attn.mat_qkv\n",
      "blocks.14.attn.proj\n",
      "blocks.14.attn.proj_drop\n",
      "blocks.14.ffn\n",
      "blocks.14.ffn.fc1\n",
      "blocks.14.ffn.act\n",
      "blocks.14.ffn.fc2\n",
      "blocks.14.ffn.drop\n",
      "blocks.14.ln_wo_grad\n",
      "blocks.14.ada_lin\n",
      "blocks.14.ada_lin.0\n",
      "blocks.14.ada_lin.1\n",
      "blocks.15\n",
      "blocks.15.drop_path\n",
      "blocks.15.attn\n",
      "blocks.15.attn.mat_qkv\n",
      "blocks.15.attn.proj\n",
      "blocks.15.attn.proj_drop\n",
      "blocks.15.ffn\n",
      "blocks.15.ffn.fc1\n",
      "blocks.15.ffn.act\n",
      "blocks.15.ffn.fc2\n",
      "blocks.15.ffn.drop\n",
      "blocks.15.ln_wo_grad\n",
      "blocks.15.ada_lin\n",
      "blocks.15.ada_lin.0\n",
      "blocks.15.ada_lin.1\n",
      "head_nm\n",
      "head_nm.ln_wo_grad\n",
      "head_nm.ada_lin\n",
      "head_nm.ada_lin.0\n",
      "head_nm.ada_lin.1\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
