{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5083e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d749914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights into the VAR model.\n",
      "Model is ready for inference.\n"
     ]
    }
   ],
   "source": [
    "var_checkpoint_path = 'var_d16.pth'\n",
    "\n",
    "# Load the state dictionary directly, as identified previously\n",
    "state_dict = torch.load(var_checkpoint_path, map_location='cpu')\n",
    "var.load_state_dict(state_dict, strict=True)\n",
    "print(\"Successfully loaded weights into the VAR model.\")\n",
    "\n",
    "# --- 4. Prepare for Inference ---\n",
    "# Set the model to evaluation mode to disable dropout, etc.\n",
    "var.eval()\n",
    "\n",
    "# The model's parameters should not require gradients for inference\n",
    "for p in var.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"Model is ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54049917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_var_activations(var_model, input_tokens, target_layers=None):\n",
    "    \"\"\"\n",
    "    Extract activations from VAR model layers during forward pass\n",
    "    \n",
    "    Args:\n",
    "        var_model: The loaded VAR model\n",
    "        input_tokens: Input tokens for the model\n",
    "        target_layers: List of layer names to extract (if None, extracts all FFN layers)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of layer_name -> activation tensor\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    if target_layers is None:\n",
    "        # Extract all FFN layers by default\n",
    "        target_layers = []\n",
    "        for name, module in var_model.named_modules():\n",
    "            if 'ffn.fc1' in name or 'ffn.fc2' in name or name == 'head':\n",
    "                target_layers.append(name)\n",
    "    \n",
    "    def make_hook(layer_name):\n",
    "        def hook_fn(module, input, output):\n",
    "            # Store activation, handling different output shapes\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                if len(output.shape) == 3:  # [batch, sequence, features]\n",
    "                    # Average across sequence dimension for transformers\n",
    "                    activations[layer_name] = output.mean(dim=1).detach()\n",
    "                else:\n",
    "                    activations[layer_name] = output.detach()\n",
    "            else:\n",
    "                # Handle tuple outputs (some layers return multiple values)\n",
    "                activations[layer_name] = output[0].detach()\n",
    "        return hook_fn\n",
    "    \n",
    "    # Register hooks\n",
    "    for layer_name in target_layers:\n",
    "        try:\n",
    "            layer = dict(var_model.named_modules())[layer_name]\n",
    "            hook = layer.register_forward_hook(make_hook(layer_name))\n",
    "            hooks.append(hook)\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Layer {layer_name} not found\")\n",
    "    \n",
    "    # Perform forward pass\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = var_model(torch.tensor([1,]), input_tokens)\n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dddfbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 256])\n",
      "tensor([[2248]])\n",
      "torch.Size([1, 679, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/Programmiersachen/Uni/PhD-Applications/cispa-prep/code/VAR/models/var.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "image_path = \"../data/imagenette/n01440764/ILSVRC2012_val_00009111.JPEG\"\n",
    "# Load and preprocess image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image to tokens using VAE\n",
    "with torch.no_grad():\n",
    "    # This depends on your VAE implementation\n",
    "    # You may need to adjust based on your specific VAE interface\n",
    "    #encoded_tokens = vae.encode(image_tensor)  # Adjust this line\n",
    "    vae_out = vae.img_to_idxBl(image_tensor)\n",
    "    var_in = var.vae_quant_proxy[0].idxBl_to_var_input(vae_out)\n",
    "print(\"shapes\")\n",
    "for el in vae_out:\n",
    "    print(el.shape)\n",
    "print(vae_out[0])\n",
    "print(var_in.shape)\n",
    "\n",
    "# Extract activations from VAR\n",
    "activations = extract_var_activations(var, var_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5d5aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: blocks.0.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.74638313 -2.9305754  -3.013914   ... -0.52451587 -2.7052476\n",
      "  -0.49910337]]\n",
      "Layer: blocks.0.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 1.4226226e-04  3.6634475e-02 -6.1239654e-01 ... -8.0569319e-02\n",
      "   7.6975203e-03  1.5197721e-01]]\n",
      "Layer: blocks.1.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.33342698 -0.1417509  -1.0790021  ... -2.398618   -1.7207677\n",
      "  -0.43498006]]\n",
      "Layer: blocks.1.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.12657642  0.08855741 -0.22504513 ... -0.0940779   0.3302756\n",
      "  -0.2608851 ]]\n",
      "Layer: blocks.2.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.6254736 -1.173401  -0.6255016 ... -1.00569   -2.9676414 -1.9144794]]\n",
      "Layer: blocks.2.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.08841949 -0.15493914  0.06704925 ... -0.09750675  0.12207684\n",
      "   0.01863836]]\n",
      "Layer: blocks.3.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-1.5254282  -1.3557339  -0.42768756 ... -0.43645105 -0.6304378\n",
      "  -0.5071911 ]]\n",
      "Layer: blocks.3.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.16190216 -0.10365634  0.26408392 ... -0.36636022 -0.10980278\n",
      "   0.1527394 ]]\n",
      "Layer: blocks.4.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.09345774 -1.5277865  -0.87317675 ... -0.7589725  -0.77376676\n",
      "  -1.2141143 ]]\n",
      "Layer: blocks.4.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[0.01875312 0.1419233  0.14840518 ... 0.28757885 0.11493035 0.03475268]]\n",
      "Layer: blocks.5.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.17824519 -0.2041653  -3.2797644  ... -0.6864229  -0.47272313\n",
      "  -1.1581008 ]]\n",
      "Layer: blocks.5.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[-0.03062724  0.19197229  0.24757493 ...  0.20266616  0.1680373\n",
      "   0.00583497]]\n",
      "Layer: blocks.6.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-1.4064342  -0.649962   -0.8008384  ... -1.0306162  -0.55646104\n",
      "  -1.1132227 ]]\n",
      "Layer: blocks.6.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[-0.04630514  0.15319811 -0.00810602 ...  0.15947868  0.21876796\n",
      "   0.10029843]]\n",
      "Layer: blocks.7.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.8926834  -0.23726876 -0.09749854 ... -0.67839533 -0.27635473\n",
      "  -0.25819284]]\n",
      "Layer: blocks.7.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.10335712 -0.2088215   0.06406949 ... -0.18613935  0.02148256\n",
      "   0.04230199]]\n",
      "Layer: blocks.8.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-1.0381653  -1.0677379  -0.7664738  ... -0.7674752  -0.62887853\n",
      "  -0.6404691 ]]\n",
      "Layer: blocks.8.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.02852453 -0.08248496  0.08412664 ... -0.21724634 -0.17701963\n",
      "   0.00070934]]\n",
      "Layer: blocks.9.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.62998515  0.22251895 -1.1607108  ... -1.3663671  -1.2432691\n",
      "  -0.49572396]]\n",
      "Layer: blocks.9.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[-0.1003745   0.06387664  0.02645336 ...  0.08129101 -0.01978162\n",
      "   0.13230616]]\n",
      "Layer: blocks.10.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-1.2926158  -0.15392242 -0.5820392  ... -0.6806002  -1.0124527\n",
      "  -0.28183526]]\n",
      "Layer: blocks.10.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[-0.06874392  0.08437064 -0.09271389 ... -0.02774352 -0.03798933\n",
      "   0.07537609]]\n",
      "Layer: blocks.11.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-1.075095   -0.5716687  -0.93127704 ...  0.01586041 -0.7296623\n",
      "  -0.7002947 ]]\n",
      "Layer: blocks.11.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.04122201  0.04139162 -0.04867151 ...  0.09366965 -0.00790193\n",
      "   0.04616297]]\n",
      "Layer: blocks.12.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.94707894 -0.29606155 -1.4741622  ... -0.6642499  -0.86330235\n",
      "  -0.6616402 ]]\n",
      "Layer: blocks.12.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.05998359 -0.07712229  0.12039129 ...  0.07701421  0.07870471\n",
      "   0.02540781]]\n",
      "Layer: blocks.13.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.4814322  -0.4519821   0.00939195 ... -0.7206494  -1.2116761\n",
      "  -2.0728455 ]]\n",
      "Layer: blocks.13.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[-0.13057974 -0.07872722 -0.11118035 ...  0.02315864 -0.10237452\n",
      "  -0.12113378]]\n",
      "Layer: blocks.14.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.8686686 -1.131368  -1.2872578 ... -0.7977238 -1.7247443 -1.5028288]]\n",
      "Layer: blocks.14.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 0.01580045  0.02492446  0.273068   ... -0.16420488  0.21282153\n",
      "   0.00348167]]\n",
      "Layer: blocks.15.ffn.fc1, Activation shape: torch.Size([1, 4096])\n",
      "[[-0.1354244  -0.3490372   0.14924575 ... -0.28600773 -0.21644334\n",
      "  -0.43272522]]\n",
      "Layer: blocks.15.ffn.fc2, Activation shape: torch.Size([1, 1024])\n",
      "[[ 4.3929446e-02 -7.1067914e-02 -1.7395720e-04 ...  3.9692980e-01\n",
      "  -1.1244986e-01  9.1396257e-02]]\n",
      "Layer: head, Activation shape: torch.Size([1, 4096])\n",
      "[[-2.940509   2.2544968  2.405459  ...  1.6716847  1.1282605  1.3874525]]\n"
     ]
    }
   ],
   "source": [
    "for key in activations.keys():\n",
    "    print(f\"Layer: {key}, Activation shape: {activations[key].shape}\")\n",
    "    # Optionally, you can save or visualize the activations here\n",
    "    # For example, convert to numpy and save as image if needed\n",
    "    activation_image = activations[key].cpu().numpy()\n",
    "    print(activation_image)\n",
    "    # Save or visualize activation_image as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bec694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
